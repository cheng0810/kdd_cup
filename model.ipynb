{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takedata_index(csv , x):\n",
    "    import random\n",
    "    from tqdm import tqdm_notebook as tqdm\n",
    "    \n",
    "    plan = csv.plans.values\n",
    "    click = csv.click_mode.values\n",
    "\n",
    "    transport_mode = []\n",
    "    index = []\n",
    "    \n",
    "    #100% means all data\n",
    "    if(x == 100):\n",
    "        csv.reset_index(drop=True, inplace=True) \n",
    "        return csv.index\n",
    "    \n",
    "    #put the most recommend plan in to transport_mode\n",
    "    for i in range(len(plan)): \n",
    "        #it must have better way  but now i just want to quickly distingush the dif \n",
    "        #if mode = 10\n",
    "        if(str(str(plan[i]).split('transport_mode')[1])[4] == str(0)):\n",
    "            transport_mode.append(str(10))\n",
    "        #if mode = 11\n",
    "        elif(str(str(plan[i]).split('transport_mode')[1])[4] == str(1)):\n",
    "            transport_mode.append(str(11))\n",
    "        else:\n",
    "            transport_mode.append(str(str(plan[i]).split('transport_mode')[1])[3])\n",
    "    \n",
    "        \n",
    "    #find dif index between transport_mode and click_mode\n",
    "    for i in range(len(click)): \n",
    "        if(int(transport_mode[i]) != click[i]):\n",
    "            index.append(i)\n",
    "    \n",
    "    #put most recommend_mode people __% to training\n",
    "    dif_id_len = len(index)\n",
    "    normal_id_len = int(dif_id_len* (x/100))\n",
    "    for i in tqdm(range(normal_id_len)):\n",
    "        num = random.randint(0,len(plan)-1)\n",
    "        while(num in index):\n",
    "            num = random.randint(0,len(plan)-1)\n",
    "        index.append(num)\n",
    "    index.sort()\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(file, index):\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    train_d = file.iloc[index]\n",
    "    train_label = train_d.click_mode\n",
    "    #choose what feature you don't need\n",
    "    train_d = train_d.drop(columns= ['sid','pid','plans','req_time','plan_time','o','d','click_mode','click_time'])\n",
    "    #one hot 'time_hour'\n",
    "    df2 = pd.get_dummies(train_d['time_hour'])\n",
    "    train_d = train_d.join(df2)\n",
    "    train_d = train_d.drop(columns = ['time_hour'])\n",
    "    train_d.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    #normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    train_d = scaler.fit_transform(train_d)\n",
    "    train_d = pd.DataFrame(train_d)\n",
    "    \n",
    "    return train_d, train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf(data, label):\n",
    "    from sklearn import preprocessing, metrics, ensemble\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.externals import joblib #save model\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.metrics import f1_score\n",
    "\n",
    "    # parameters for GridSearchCV\n",
    "    param_grid = {\"n_estimators\": [100, 200,300],\n",
    "#                   \"max_depth\": [5,7,10],\n",
    "#                   \"min_samples_split\": [2,5,10],\n",
    "#                   \"min_samples_leaf\": [1, 5, 10],\n",
    "#                   \"max_leaf_nodes\": [20, 50, 100],\n",
    "                  \"min_weight_fraction_leaf\": [0, 0.1]}\n",
    "   \n",
    "    \n",
    "    train_X, test_X, train_y, test_y = train_test_split(data, label, test_size = 0.3)\n",
    "    #grid search\n",
    "    forest = ensemble.RandomForestClassifier()\n",
    "    grid_search = GridSearchCV(estimator = forest,param_grid = param_grid, n_jobs = 1, verbose=3, cv = 5)    \n",
    "    grid_search.fit(train_X, train_y)\n",
    "    print(grid_search.best_params_)\n",
    "    #train randomforest\n",
    "    forest = ensemble.RandomForestClassifier(**grid_search.best_params_)\n",
    "    forest_fit = forest.fit(train_X, train_y)\n",
    "    \n",
    "    test_y_predicted = forest.predict(test_X)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    f1 = f1_score(test_y, test_y_predicted, average='weighted')\n",
    "    print(forest_fit)\n",
    "    print(accuracy)\n",
    "    print(f1)\n",
    "    joblib.dump(forest, 'rf5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(data, label):\n",
    "    from sklearn import preprocessing, cross_validation, metrics, ensemble\n",
    "    from sklearn.externals import joblib #save model\n",
    "    from xgboost.sklearn import XGBClassifier\n",
    "    from sklearn.metrics import f1_score\n",
    "    \n",
    "    train_X, test_X, train_y, test_y = cross_validation.train_test_split(data, label, test_size = 0.3)\n",
    "    xgbc = XGBClassifier(silent=1)\n",
    "    xgbc_fit = xgbc.fit(train_X, train_y)\n",
    "    test_y_predicted = xgbc.predict(test_X)\n",
    "\n",
    "    accuracy = metrics.accuracy_score(test_y, test_y_predicted)\n",
    "    f1 = f1_score(test_y, test_y_predicted, average='weighted')\n",
    "    print(xgbc_fit)\n",
    "    print(accuracy)\n",
    "    print(f1)\n",
    "    joblib.dump(xgbc, 'xgbc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import csv\n",
    "    \n",
    "    train_final = pd.read_csv('/storage/kdd/train_final2.csv')\n",
    "    \n",
    "    index = takedata_index(train_final ,100)\n",
    "    train_d, train_label = train_data(train_final, index)\n",
    "    rf(train_d, train_label)\n",
    "#     xgboost(train_d, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0, score=0.696370052936728, total= 1.7min\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0, score=0.6960563424664009, total= 1.7min\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0 ....................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0, score=0.6935839662181326, total= 1.7min\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0, score=0.6969510753958875, total= 1.7min\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0, score=0.697445755794715, total= 1.7min\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0, score=0.6967639274010587, total= 3.4min\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0, score=0.6983566780633065, total= 3.4min\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0, score=0.6959474364226514, total= 3.4min\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0, score=0.6968722918143859, total= 3.4min\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0, score=0.6986433039723934, total= 3.4min\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0, score=0.6975516763297201, total= 5.1min\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0, score=0.6993177771825616, total= 5.1min\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0, score=0.6968928245044591, total= 5.1min\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0, score=0.6979437485228078, total= 5.1min\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0 ....................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0, score=0.7003450829617257, total= 5.1min\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0.1, score=0.6528548021174692, total=  11.7s\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0.1, score=0.6547605917849659, total=  11.8s\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0.1, score=0.6534049727413103, total=  11.5s\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0.1, score=0.6520286772236666, total=  11.9s\n",
      "[CV] n_estimators=100, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=100, min_weight_fraction_leaf=0.1, score=0.6542079636953816, total=  11.8s\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0.1, score=0.653878875724729, total=  23.1s\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0.1, score=0.6539885613449086, total=  23.3s\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0.1, score=0.6533104339331296, total=  22.9s\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0.1, score=0.6526431891593791, total=  22.9s\n",
      "[CV] n_estimators=200, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=200, min_weight_fraction_leaf=0.1, score=0.6537037328837275, total=  23.1s\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0.1, score=0.6532644315603731, total=  34.4s\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0.1, score=0.6545715231057682, total=  34.5s\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0.1, score=0.6536885891658526, total=  34.3s\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0.1, score=0.6535413219884976, total=  33.9s\n",
      "[CV] n_estimators=300, min_weight_fraction_leaf=0.1 ..................\n",
      "[CV]  n_estimators=300, min_weight_fraction_leaf=0.1, score=0.6544443218883443, total=  34.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 64.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'min_weight_fraction_leaf': 0}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0, n_estimators=300, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n",
      "0.6991051536385762\n",
      "0.6569326222586562\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
